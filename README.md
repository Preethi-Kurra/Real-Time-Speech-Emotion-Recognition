# Speech-Emotion-Recognition
Emotions play an integral role in our daily life, understanding a person's emotions will improve human interactions, and make conversations more enjoyable. When using words, the speaker's emotion can be understood from the pitch, amplitude, and frequency among various other features of sound from the sound wave of speech. There are many fields that can take advantage of knowing one's emotional state. For example, a well-trained agent can immediately identify an upset customer and direct them to another agent who can monitor the conversation in real-time and adjust. Our goal in this project is to successfully detect emotions from an audio recording. We used machine learning and deep learning techniques like Decision Tree, Random Forest, Convolution Neural Networks (CNN), and Multi-Layer Perceptron classifier (MLP) by implementing data augmentation, feature extraction, and hyperparameter tuning. Our evaluation shows that the MLP classifier after hyperparameter tuning yields an average accuracy of 92% for 8 emotions.
We used two datasets RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song) and TESS (Toronto Emotional Speech Set). After scraping the official websites of these datasets, we found that the RAVDESS dataset has 1440 speech files of 24 professional actors (12 Females, 12 males) in North American accents categorized into calm, happy, sad, fearful, surprise, angry, disgust expressions. The TESS dataset has 2800 audio files where a set of 200 words were spoken by 2 actresses aged 26 and 64 years.
